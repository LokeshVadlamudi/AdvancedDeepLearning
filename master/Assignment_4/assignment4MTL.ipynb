{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment4MTL.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1VWVA6OFAoOSZJnt0V6anHB103Ffi6fSQ",
      "authorship_tag": "ABX9TyNm3KkyLJIemLKkabOUfXDe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LokeshVadlamudi/EmergingTechnologiesML/blob/master/master/Assignment_4/assignment4MTL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F67ggGULK5Xb",
        "outputId": "c50e3040-eeea-404a-e624-83e41737d65a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "!pip3 uninstall keras\n",
        "!pip3 install keras --upgrade"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling Keras-2.4.3:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/Keras-2.4.3.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/*\n",
            "    /usr/local/lib/python3.6/dist-packages/keras/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/md_autogen.py\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/update_docs.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Keras-2.4.3\n",
            "Collecting keras\n",
            "  Downloading https://files.pythonhosted.org/packages/44/e1/dc0757b20b56c980b5553c1b5c4c32d378c7055ab7bfa92006801ad359ab/Keras-2.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.4.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ6xq6wIRv_O"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras import backend as K\n",
        "from keras.layers import Input\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "import random \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from random import randrange\n",
        "import cv2\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg19 import preprocess_input\n",
        "from keras.layers import Activation, Flatten, Dropout, BatchNormalization\n",
        "\n",
        "# K.tensorflow_backend._get_available_gpus()\n",
        "\n",
        "\n",
        "dat = pd.read_csv('/content/drive/My Drive/MTL assignment/fgo_multiclass_labels.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc-t0VXQKpGQ",
        "outputId": "d7152edb-edf2-403d-8f66-aa84d0329e64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "dat.head()\n",
        "\n",
        "dat.columns"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'full_path', 'image_name', 'name', 'white', 'red',\n",
              "       'green', 'black', 'blue', 'purple', 'gold', 'silver', 'gender_Female',\n",
              "       'gender_Male', 'region_Asia', 'region_Egypt', 'region_Europe',\n",
              "       'region_Middle East', 'fighting_type_magic', 'fighting_type_melee',\n",
              "       'fighting_type_ranged', 'alignment_CE', 'alignment_CG', 'alignment_CN',\n",
              "       'alignment_LE', 'alignment_LG', 'alignment_LN', 'alignment_NE',\n",
              "       'alignment_NG', 'alignment_TN'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj1NDeDXLrwc"
      },
      "source": [
        "X = dat['image_name']\n",
        "y = dat[['white', 'red',\n",
        "       'green', 'black', 'blue', 'purple', 'gold', 'silver', 'gender_Female',\n",
        "       'gender_Male', 'region_Asia', 'region_Egypt', 'region_Europe',\n",
        "       'region_Middle East', 'fighting_type_magic', 'fighting_type_melee',\n",
        "       'fighting_type_ranged', 'alignment_CE', 'alignment_CG', 'alignment_CN',\n",
        "       'alignment_LE', 'alignment_LG', 'alignment_LN', 'alignment_NE',\n",
        "       'alignment_NG', 'alignment_TN']]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3mAcjyzLwOH",
        "outputId": "03b883b7-fe15-47c8-ffd2-23eac6bc7a21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_train.columns\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['white', 'red', 'green', 'black', 'blue', 'purple', 'gold', 'silver',\n",
              "       'gender_Female', 'gender_Male', 'region_Asia', 'region_Egypt',\n",
              "       'region_Europe', 'region_Middle East', 'fighting_type_magic',\n",
              "       'fighting_type_melee', 'fighting_type_ranged', 'alignment_CE',\n",
              "       'alignment_CG', 'alignment_CN', 'alignment_LE', 'alignment_LG',\n",
              "       'alignment_LN', 'alignment_NE', 'alignment_NG', 'alignment_TN'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etiBVYdxLxpr",
        "outputId": "98d55783-b755-4eb5-cd0a-e7ce435ae174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train = X_train.values.tolist()\n",
        "X_test = X_test.values.tolist()\n",
        "\n",
        "#train\n",
        "colors = ['white', 'red',\n",
        "       'green', 'black', 'blue', 'purple', 'gold', 'silver']\n",
        "color_train = y_train[colors]\n",
        "color_nodes = color_train.shape[1]\n",
        "color_train = color_train.values.tolist()\n",
        "\n",
        "gender = ['gender_Female',\n",
        "       'gender_Male']\n",
        "gender_train = y_train[gender]\n",
        "gender_nodes = gender_train.shape[1]\n",
        "gender_train = gender_train.values.tolist()\n",
        "\n",
        "region = ['region_Asia', 'region_Egypt', 'region_Europe',\n",
        "       'region_Middle East']\n",
        "region_train = y_train[region]\n",
        "region_nodes = region_train.shape[1]\n",
        "region_train = region_train.values.tolist()\n",
        "\n",
        "fighting_style = ['fighting_type_magic', 'fighting_type_melee',\n",
        "       'fighting_type_ranged']\n",
        "fighting_style_train = y_train[fighting_style]\n",
        "fighting_nodes = fighting_style_train.shape[1]\n",
        "fighting_style_train = fighting_style_train.values.tolist()\n",
        "\n",
        "alignment = ['alignment_CE', 'alignment_CG', 'alignment_CN',\n",
        "       'alignment_LE', 'alignment_LG', 'alignment_LN', 'alignment_NE',\n",
        "       'alignment_NG', 'alignment_TN']\n",
        "alignment_train = y_train[alignment]\n",
        "alignment_nodes = alignment_train.shape[1]\n",
        "alignment_train = alignment_train.values.tolist()\n",
        "\n",
        "print(color_nodes,gender_nodes,region_nodes,fighting_nodes,alignment_nodes)\n",
        "#test\n",
        "colors = ['white', 'red',\n",
        "       'green', 'black', 'blue', 'purple', 'gold', 'silver']\n",
        "color_test = y_test[colors]\n",
        "color_nodes = color_test.shape[1]\n",
        "color_test = color_test.values.tolist()\n",
        "\n",
        "gender = ['gender_Female',\n",
        "       'gender_Male']\n",
        "gender_test = y_test[gender]\n",
        "gender_nodes = gender_test.shape[1]\n",
        "gender_test = gender_test.values.tolist()\n",
        "\n",
        "region = ['region_Asia', 'region_Egypt', 'region_Europe',\n",
        "       'region_Middle East']\n",
        "region_test = y_test[region]\n",
        "region_nodes = region_test.shape[1]\n",
        "region_test = region_test.values.tolist()\n",
        "\n",
        "fighting_style = ['fighting_type_magic', 'fighting_type_melee',\n",
        "       'fighting_type_ranged']\n",
        "fighting_style_test = y_test[fighting_style]\n",
        "fighting_nodes = fighting_style_test.shape[1]\n",
        "fighting_style_test = fighting_style_test.values.tolist()\n",
        "\n",
        "alignment = ['alignment_CE', 'alignment_CG', 'alignment_CN',\n",
        "       'alignment_LE', 'alignment_LG', 'alignment_LN', 'alignment_NE',\n",
        "       'alignment_NG', 'alignment_TN']\n",
        "alignment_test = y_test[alignment]\n",
        "alignment_nodes = alignment_test.shape[1]\n",
        "alignment_test = alignment_test.values.tolist()\n",
        "\n",
        "print(color_nodes,gender_nodes,region_nodes,fighting_nodes,alignment_nodes)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8 2 4 3 9\n",
            "8 2 4 3 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoELHYgwL2Bf"
      },
      "source": [
        "# initialize the number of epochs and batch size\n",
        "EPOCHS = 50\n",
        "BS = 16\n",
        "\n",
        "\n",
        "def image_generator_fgo(king_of_lists, bs, mode=\"train\", aug=None):\n",
        "    # loop indefinitely\n",
        "    \n",
        "    while True:\n",
        "        # initialize our batches of images and labels\n",
        "        images = []\n",
        "        #labels = []\n",
        "        \n",
        "        color = []\n",
        "        gender = []\n",
        "        region = []\n",
        "        fight = []\n",
        "        alignment = []\n",
        "        \n",
        "        # keep looping until we reach our batch size\n",
        "        while len(images) < bs:\n",
        "            combined_label_list = []\n",
        "            random_index = randrange(len(king_of_lists[0]))\n",
        "            img = image.load_img('images/'+king_of_lists[0][random_index],target_size=(224, 224)) #read in image\n",
        "            img = image.img_to_array(img)\n",
        "            #img = cv2.resize(img, (224, 224))\n",
        "            #F this making my own augmentations\n",
        "            #rand = random.randint(1,101)\n",
        "            #if rand < 50: \n",
        "            #    img = cv2.flip( img, 0 )# horizantal flip\n",
        "            #rand = random.randint(1,101)\n",
        "            #img = np.expand_dims(img, axis=0)\n",
        "            img = preprocess_input(img)\n",
        "            \n",
        "            #create labels\n",
        "            gender.append(king_of_lists[1][random_index]) # gender\n",
        "            region.append(np.array(king_of_lists[2][random_index])) # region\n",
        "            fight.append(np.array(king_of_lists[3][random_index])) # fighting\n",
        "            alignment.append(np.array(king_of_lists[4][random_index])) # alignment\n",
        "            color.append(np.array(king_of_lists[5][random_index])) # color\n",
        "            \n",
        "            images.append(img)\n",
        "            #labels.append(gender\n",
        "            \n",
        "        #labels = {'gender': np.array(gender), 'region': np.array(region),\n",
        "        #        'fighting_style': np.array(fight),\n",
        "        #         'alignment': np.array(alignment),'color': np.array(color)}\n",
        "        labels = [np.array(gender),np.array(region),np.array(fight),np.array(alignment), np.array(color)]\n",
        "        #labels = [gender,region,fight,alignment,color]\n",
        "        # if the data augmentation object is not None, apply it\n",
        "        #labels\n",
        "        if aug is not None:\n",
        "            (images, labels) = next(aug.flow(np.array(images),labels, batch_size=bs))\n",
        "        \n",
        "        #print(labels.shape)\n",
        "        # yield the batch to the calling function\n",
        "        yield np.array(images),  labels \n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "                         width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "train_lists = [X_train, gender_train, region_train, fighting_style_train, alignment_train, color_train]\n",
        "test_lists = [X_test, gender_test, region_test, fighting_style_test, alignment_test, color_test]\n",
        "#train_lists = [X_train, region_train, fighting_style_train, alignment_train, color_train]\n",
        "#test_lists = [X_test, region_test, fighting_style_test, alignment_test, color_test]\n",
        "# initialize both the training and testing image generators\n",
        "trainGen = image_generator_fgo(train_lists, BS, \n",
        "                               mode=\"train\", aug=None)\n",
        "testGen = image_generator_fgo(test_lists, BS, \n",
        "                              mode=\"train\", aug=None)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU7YZacSL7iI",
        "outputId": "a164d713-fac6-4e52-fa58-e803bff708dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Color Images, Multi-Label Targets\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "loss_list = [\"categorical_crossentropy\",'categorical_crossentropy',\n",
        "'categorical_crossentropy','categorical_crossentropy',\n",
        "'binary_crossentropy']\n",
        "\n",
        "test_metrics = {'gender': 'accuracy','region': 'accuracy',\n",
        "               'fighting_style': 'accuracy','alignment': 'accuracy',\n",
        "               'color': 'accuracy'}\n",
        "dd = 0.0\n",
        "import math\n",
        "def step_decay(epoch):\n",
        "   initial_lrate = 0.01\n",
        "   drop = 0.5\n",
        "   epochs_drop = 5.0\n",
        "   lrate = initial_lrate * math.pow(drop,  \n",
        "           math.floor((1+epoch)/epochs_drop))\n",
        "   return lrate\n",
        "\n",
        "def multi_model(loss_list,test_metrics,dd):\n",
        "    \n",
        "    base_model = VGG19(weights='imagenet', include_top=False)\n",
        "\n",
        "    #freeze all the layers\n",
        "    for layer in base_model.layers[:]:\n",
        "       layer.trainable = False\n",
        "\n",
        "    \n",
        "    model_input = Input(shape=(224, 224, 3))\n",
        "    x = base_model(model_input)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    \n",
        "    # let's add a fully-connected layer\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(dd)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(dd)(x)\n",
        "    # start passing that fully connected block output to all the \n",
        "    # different model heads\n",
        "    y1 = Dense(128, activation='relu')(x)\n",
        "    y1 = Dropout(dd)(y1)\n",
        "    y1 = Dense(64, activation='relu')(y1)\n",
        "    y1 = Dropout(dd)(y1)\n",
        "    \n",
        "    y2 = Dense(128, activation='relu')(x)\n",
        "    y2 = Dropout(dd)(y2)\n",
        "    y2 = Dense(64, activation='relu')(y2)\n",
        "    y2 = Dropout(dd)(y2)\n",
        "    \n",
        "    y3 = Dense(128, activation='relu')(x)\n",
        "    y3 = Dropout(dd)(y3)\n",
        "    y3 = Dense(64, activation='relu')(y3)\n",
        "    y3 = Dropout(dd)(y3)\n",
        "\n",
        "    y4 = Dense(128, activation='relu')(x)\n",
        "    y4 = Dropout(dd)(y4)\n",
        "    y4 = Dense(64, activation='relu')(y4)\n",
        "    y4 = Dropout(dd)(y4)\n",
        "    \n",
        "    y5 = Dense(128, activation='relu')(x)\n",
        "    y5 = Dropout(dd)(y5)\n",
        "    y5 = Dense(64, activation='relu')(y5)\n",
        "    y5 = Dropout(dd)(y5)\n",
        "    \n",
        "    #connect all the heads to their final output layers\n",
        "    y1 = Dense(gender_nodes, activation='softmax',name= 'gender')(y1)\n",
        "    y2 = Dense(region_nodes, activation='softmax',name= 'region')(y2)\n",
        "    y3 = Dense(fighting_nodes, activation='softmax',name= 'fighting_style')(y3)\n",
        "    y4 = Dense(alignment_nodes, activation='softmax',name= 'alignment')(y4)\n",
        "    y5 = Dense(color_nodes, activation='sigmoid',name= 'color')(y5)\n",
        "    \n",
        "    model = Model(inputs=model_input, outputs=[ y1, y2, y3, y4, y5])\n",
        "    \n",
        "    model.compile(loss=loss_list, optimizer=SGD(lr=0.01,momentum=0.9), metrics=test_metrics)\n",
        "\n",
        "    return model\n",
        "\n",
        "multi_model = multi_model(loss_list,test_metrics,dd)\n",
        "\n",
        "multi_model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "vgg19 (Functional)              (None, None, None, 5 20024384    input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 512)          0           vgg19[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          131328      global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          65792       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 128)          32896       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 128)          32896       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 128)          0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 128)          0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 128)          0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 64)           8256        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 64)           8256        dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 64)           8256        dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 64)           8256        dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 64)           8256        dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 64)           0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 64)           0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 64)           0           dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 64)           0           dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 64)           0           dense_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "gender (Dense)                  (None, 2)            130         dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "region (Dense)                  (None, 4)            260         dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "fighting_style (Dense)          (None, 3)            195         dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "alignment (Dense)               (None, 9)            585         dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "color (Dense)                   (None, 8)            520         dropout_11[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 20,428,954\n",
            "Trainable params: 404,570\n",
            "Non-trainable params: 20,024,384\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTh9xNlvMBdl",
        "outputId": "6eeafa6e-62c8-44a3-dd2d-f9ee21c9fa78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "checkpoint = ModelCheckpoint('models/best_run5_smaller.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "lrate = LearningRateScheduler(step_decay)\n",
        "callbacks_list = [checkpoint,lrate]\n",
        "multi_model.fit_generator(trainGen,steps_per_epoch=len(X_train) // BS,\n",
        "                    validation_data=testGen,\n",
        "                    validation_steps=len(X_test) // BS,\n",
        "                    epochs=EPOCHS,callbacks=callbacks_list)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-d5c2f886292e>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d5c2f886292e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestGen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     epochs=EPOCHS,callbacks=callbacks_list)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[0;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m     \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    841\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-5793d4d21f0a>\u001b[0m in \u001b[0;36mimage_generator_fgo\u001b[0;34m(king_of_lists, bs, mode, aug)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mcombined_label_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mrandom_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mking_of_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mking_of_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#read in image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m#img = cv2.resize(img, (224, 224))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    299\u001b[0m   \"\"\"\n\u001b[1;32m    300\u001b[0m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0;32m--> 301\u001b[0;31m                         target_size=target_size, interpolation=interpolation)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images/285leIA.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-vhyEebMGp3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}